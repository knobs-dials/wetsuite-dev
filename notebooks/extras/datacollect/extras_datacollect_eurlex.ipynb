{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scarfboy/wetsuite-dev/blob/main/examples/datacollect_koop_repos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rCIrIWma5Omg"
      },
      "source": [
        "## This notebook's goal\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Figuring out how to get data out of EUR-Lex. \n",
        "\n",
        "Currently aimed specifically at the court judgments, and then mainly the text."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are [a few different ways to access different parts of EUR-Lex data](https://eur-lex.europa.eu/content/welcome/data-reuse.html),\n",
        "including a RESTful API, a SOAP API (requires registration), and a SPARQL endpoint.\n",
        "\n",
        "Probably the most flexible is the SPARQL endpoint,\n",
        "particularly when looking for specific selections of documents, specific relations, and such.\n",
        "At the same time, SPARQL presents a bit of a learning curve unless you're already hardcore into RDF.\n",
        "\n",
        "<!-- -->\n",
        "\n",
        "SPARQL results refer to a work that is mostly the content text as HTML, e.g. http://publications.europa.eu/resource/cellar/1e3100ce-8a71-433a-8135-15f5cc0e927c.0002.02/DOC_1\n",
        "Actually, the public-facing web page describing the thing (by CELEX), e.g. https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX%3A61996CJ0080\n",
        "gives even better detail,\n",
        "- links to the underlying document\n",
        "- ...for all translated languages\n",
        "- the text\n",
        "- more metadata, like classification, related documents\n",
        "\n",
        "...so for first experiments, and before learning SPARQL, we could read of details from there.\n",
        "If we do, we still need a source of CELEX identifers to know what to fetch. The SPARQL endpoint is still quite useful for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZfK62wjZnmxc"
      },
      "outputs": [],
      "source": [
        "import random, pprint, json, random, importlib\n",
        "\n",
        "import tqdm\n",
        "\n",
        "import wetsuite.helpers.localdata\n",
        "import wetsuite.helpers.etree\n",
        "import wetsuite.helpers.eurlex\n",
        "import wetsuite.helpers.net"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Judgments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "judgment_celexes = wetsuite.helpers.localdata.LocalKV('eurlex_judg_celex_workid.db', key_type=str,value_type=str)   # stores CELEX -> work id       (mostly just for the CELEX)\n",
        "judgment_docs_en = wetsuite.helpers.localdata.LocalKV('eurlex_judg_en.db', key_type=str,value_type=bytes)           # stores url -> html document\n",
        "judgment_docs_nl = wetsuite.helpers.localdata.LocalKV('eurlex_judg_nl.db', key_type=str,value_type=bytes)           # stores url -> html document"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch identifiers and documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "judg_dict = wetsuite.helpers.eurlex.fetch_by_resource_type('JUDG') # as of this writing there are 27K results (roughly 4GB worth of HTML)\n",
        "judg_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fetch just the fact that the CELEX identifiers exist   (also the workid they point to, though we don't use that yet)\n",
        "for work in judg_dict['results']['bindings']:\n",
        "    try:\n",
        "        celex  = work['celex']['value']\n",
        "        workid = work['work']['value']\n",
        "        judgment_celexes.put(celex, workid)\n",
        "        #judgment_celexes.put(celex, workid, commit=False)  \n",
        "    except KeyError as ke:\n",
        "        print( 'missing %s: %s'%(str(ke), work) )\n",
        "#judgment_celexes.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'wetsuite' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/var/www/coding/wetsuite/notebooks/extras/datacollect/extras_datacollect_eurlex.ipynb Cell 11\u001b[0m line \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3137382e323139222c2275736572223a227363617266626f79227d/var/www/coding/wetsuite/notebooks/extras/datacollect/extras_datacollect_eurlex.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# fetch the web pages for all those CELEXes, for EN\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3137382e323139222c2275736572223a227363617266626f79227d/var/www/coding/wetsuite/notebooks/extras/datacollect/extras_datacollect_eurlex.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m pbar \u001b[39m=\u001b[39m wetsuite\u001b[39m.\u001b[39mhelpers\u001b[39m.\u001b[39mnotebook\u001b[39m.\u001b[39mprogress_bar( \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(judgment_celexes), description\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfetching pages...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3137382e323139222c2275736572223a227363617266626f79227d/var/www/coding/wetsuite/notebooks/extras/datacollect/extras_datacollect_eurlex.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m count_cached, count_fetched \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3137382e323139222c2275736572223a227363617266626f79227d/var/www/coding/wetsuite/notebooks/extras/datacollect/extras_datacollect_eurlex.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m celex \u001b[39min\u001b[39;00m judgment_celexes:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3137382e323139222c2275736572223a227363617266626f79227d/var/www/coding/wetsuite/notebooks/extras/datacollect/extras_datacollect_eurlex.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# the /ALL/ page gives more metadata than e.g. AUTO, TXT, though we might be interested in fetching specific-language \u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wetsuite' is not defined"
          ]
        }
      ],
      "source": [
        "# fetch the web pages for all those CELEXes, for EN\n",
        "\n",
        "pbar = wetsuite.helpers.notebook.progress_bar( max=len(judgment_celexes), description='fetching pages...')\n",
        "count_cached, count_fetched = 0, 0\n",
        "\n",
        "for celex in judgment_celexes:\n",
        "    # the /ALL/ page gives more metadata than e.g. AUTO, TXT, though we might be interested in fetching specific-language \n",
        "    if 1:\n",
        "        url = 'https://eur-lex.europa.eu/legal-content/NL/ALL/?uri=CELEX:%s'%celex\n",
        "        try:\n",
        "            _, was_cached = wetsuite.helpers.localdata.cached_fetch( judgment_docs_nl, url )\n",
        "            if was_cached:\n",
        "                count_cached += 1\n",
        "            else:\n",
        "                count_fetched += 1\n",
        "        except Exception as e:\n",
        "            print( e, url )\n",
        "\n",
        "    if 0:\n",
        "        url = 'https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX:%s'%celex\n",
        "        try:\n",
        "            wetsuite.helpers.localdata.cached_fetch( judgment_docs_en, url )\n",
        "        except Exception as e: # it seems the server will report overloads as 404, so running it another time should\n",
        "            print( e, url )\n",
        "    \n",
        "    pbar.value += 1\n",
        "    pbar.description = f'{count_fetched} fetched, {count_cached} cached'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test parsing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# debug, don't store yet\n",
        "importlib.reload(wetsuite.helpers.eurlex)\n",
        "\n",
        "for url in random.sample( judgment_docs_nl.keys(), 10 ): # pick a bunch of random documents, \n",
        "    random_doc = judgment_docs_nl[ url ]\n",
        "    try:\n",
        "        #print(url)\n",
        "        parsed = wetsuite.helpers.eurlex.extract_html(random_doc)   # that function is where most of the scraping code sits\n",
        "        #pprint.pprint( parsed['text'] )\n",
        "    except Exception as e:\n",
        "        print( url )\n",
        "        raise"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Did that give good text and not error out?   Then we can probably run it on the whole set and store the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "parsed_store = wetsuite.helpers.localdata.LocalKV('eurlex_parsed.db', key_type=str,value_type=str)    # stores CELEX -> json as str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parse and store\n",
        "importlib.reload(wetsuite.helpers.eurlex)\n",
        "\n",
        "keys = judgment_docs_nl.keys()\n",
        "\n",
        "force = False\n",
        "random.shuffle(keys)\n",
        "lk = len(keys)\n",
        "for url in tqdm.tqdm( keys, unit='page' ):\n",
        "    if url not in parsed_store  or  force:\n",
        "        docbytes = judgment_docs_nl[ url ]\n",
        "        try:\n",
        "            parsed = wetsuite.helpers.eurlex.extract_html( docbytes )\n",
        "            parsed_store.put( url, json.dumps( parsed ) )\n",
        "        except Exception as e:\n",
        "            print( url )\n",
        "            pprint.pprint( parsed )\n",
        "            raise"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Regulations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "reg_celexes = wetsuite.helpers.localdata.LocalKV('eurlex_reg_celex_workid.db', key_type=str,value_type=str)     # stores CELEX -> work id       (mostly just for the CELEX)\n",
        "reg_docs_en = wetsuite.helpers.localdata.LocalKV('eurlex_reg_en.db', key_type=str,value_type=bytes)   # stores url -> html document\n",
        "reg_docs_nl = wetsuite.helpers.localdata.LocalKV('eurlex_reg_nl.db', key_type=str,value_type=bytes)   # stores url -> html document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch current list\n",
        "reg_dict = wetsuite.helpers.eurlex.fetch_by_resource_type('REG') # as of this writing there are 130K results (roughly GB worth of HTML)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# take that fetched state and update (mainly) the fact that the CELEX identifiers exist   (also the workid they point to, though we don't use that yet)\n",
        "for work in reg_dict['results']['bindings']:\n",
        "    try:\n",
        "        celex  = work['celex']['value']\n",
        "        workid = work['work']['value']\n",
        "        reg_celexes.put(celex, workid, commit=False)  \n",
        "    except KeyError as ke:\n",
        "        print( 'missing %s: %s'%(str(ke), work) )\n",
        "reg_celexes.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(123220, 130031)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(reg_docs_nl), len(reg_celexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 130031/130031 [4:54:41<00:00,  7.35it/s]   \n"
          ]
        }
      ],
      "source": [
        "# fetch the web pages for all those CELEXes, for EN\n",
        "import tqdm, random\n",
        "\n",
        "kk = list(reg_celexes.keys())\n",
        "#random.shuffle(kk)\n",
        "\n",
        "for celex in tqdm.tqdm(kk):\n",
        "    # the /ALL/ page gives more metadata than e.g. AUTO, TXT, though we might be interested in fetching specific-language \n",
        "    if 1:\n",
        "        url = 'https://eur-lex.europa.eu/legal-content/NL/ALL/?uri=CELEX:%s'%celex\n",
        "        try:\n",
        "            wetsuite.helpers.localdata.cached_fetch( reg_docs_nl, url )\n",
        "            #print(url)\n",
        "        except Exception as e:\n",
        "            print( e, url )\n",
        "\n",
        "    if 0:\n",
        "        url = 'https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX:%s'%celex\n",
        "        try:\n",
        "            wetsuite.helpers.localdata.cached_fetch( reg_docs_en, url )\n",
        "        except Exception as e: # it seems the server will report overloads as 404, so running it another time should\n",
        "            print( e, url )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://eur-lex.europa.eu/legal-content/NL/ALL/?uri=CELEX:31973R0879\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "uniform() missing 2 required positional arguments: 'a' and 'b'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/var/www/coding/wetsuite/notebooks/examples/datacollect_eurlex.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3137382e323139222c2275736572223a227363617266626f79227d/var/www/coding/wetsuite/notebooks/examples/datacollect_eurlex.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3137382e323139222c2275736572223a227363617266626f79227d/var/www/coding/wetsuite/notebooks/examples/datacollect_eurlex.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m#print(url)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3137382e323139222c2275736572223a227363617266626f79227d/var/www/coding/wetsuite/notebooks/examples/datacollect_eurlex.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     parsed \u001b[39m=\u001b[39m wetsuite\u001b[39m.\u001b[39mhelpers\u001b[39m.\u001b[39meurlex\u001b[39m.\u001b[39mextract_html(random_doc)   \u001b[39m# that function is where most of the scraping code sits\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3137382e323139222c2275736572223a227363617266626f79227d/var/www/coding/wetsuite/notebooks/examples/datacollect_eurlex.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mif\u001b[39;00m random\u001b[39m.\u001b[39;49muniform()\u001b[39m<\u001b[39m\u001b[39m0.1\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3137382e323139222c2275736572223a227363617266626f79227d/var/www/coding/wetsuite/notebooks/examples/datacollect_eurlex.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         pprint\u001b[39m.\u001b[39mpprint( parsed )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3137382e323139222c2275736572223a227363617266626f79227d/var/www/coding/wetsuite/notebooks/examples/datacollect_eurlex.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m#pprint.pprint( parsed['text'] )\u001b[39;00m\n",
            "\u001b[0;31mTypeError\u001b[0m: uniform() missing 2 required positional arguments: 'a' and 'b'"
          ]
        }
      ],
      "source": [
        "# test parsing again\n",
        "#importlib.reload(wetsuite.helpers.eurlex)\n",
        "\n",
        "selection = random.sample( reg_docs_nl.keys(), 1000 )\n",
        "\n",
        "for url in tqdm.tqdm(selection): # pick 100 random documents\n",
        "    random_doc = reg_docs_nl.get( url )\n",
        "    try:\n",
        "        #print(url)\n",
        "        parsed = wetsuite.helpers.eurlex.extract_html(random_doc)   # that function is where most of the scraping code sits\n",
        "        if random.uniform(0,1)<0.05:\n",
        "            pprint.pprint( parsed )\n",
        "        #pprint.pprint( parsed['text'] )\n",
        "    except Exception as e:\n",
        "        print( url )\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
