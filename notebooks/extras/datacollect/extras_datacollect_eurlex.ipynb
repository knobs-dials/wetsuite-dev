{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/knobs-dials/wetsuite-dev/blob/main/notebooks/extras/datacollect/datacollect_eurlex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rCIrIWma5Omg"
      },
      "source": [
        "## This notebook's goal\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Figuring out how to get data out of EUR-Lex. \n",
        "\n",
        "Currently aimed specifically at the court judgments, and then mainly the text."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are [a few different ways to access different parts of EUR-Lex data](https://eur-lex.europa.eu/content/welcome/data-reuse.html),\n",
        "including a RESTful API, a SOAP API (requires registration), and a SPARQL endpoint.\n",
        "\n",
        "Probably the most flexible is the SPARQL endpoint,\n",
        "particularly when looking for specific selections of documents, specific relations, and such.\n",
        "At the same time, SPARQL presents a bit of a learning curve unless you're already hardcore into RDF.\n",
        "\n",
        "<!-- -->\n",
        "\n",
        "SPARQL results refer to a work that is mostly the content text as HTML, e.g. http://publications.europa.eu/resource/cellar/1e3100ce-8a71-433a-8135-15f5cc0e927c.0002.02/DOC_1\n",
        "Actually, the public-facing web page describing the thing (by CELEX), e.g. https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX%3A61996CJ0080\n",
        "gives even better detail,\n",
        "- links to the underlying document\n",
        "- ...for all translated languages\n",
        "- the text\n",
        "- more metadata, like classification, related documents\n",
        "\n",
        "...so for first experiments, and before learning SPARQL, we could read of details from there.\n",
        "If we do, we still need a source of CELEX identifers to know what to fetch. The SPARQL endpoint is still quite useful for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZfK62wjZnmxc"
      },
      "outputs": [],
      "source": [
        "import random, pprint, json, random, time\n",
        "\n",
        "import tqdm\n",
        "\n",
        "import wetsuite.datacollect.eurlex\n",
        "import wetsuite.helpers.notebook\n",
        "import wetsuite.helpers.localdata\n",
        "import wetsuite.helpers.etree\n",
        "import wetsuite.helpers.net"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Judgments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "judgment_celexes = wetsuite.helpers.localdata.LocalKV('eurlex_judg_celex_workid.db', key_type=str,value_type=str)   # stores CELEX -> work id       (mostly just for the CELEX)\n",
        "judgment_docs_en = wetsuite.helpers.localdata.LocalKV('eurlex_judg_en.db', key_type=str,value_type=bytes)           # stores url -> html document\n",
        "judgment_docs_nl = wetsuite.helpers.localdata.LocalKV('eurlex_judg_nl.db', key_type=str,value_type=bytes)           # stores url -> html document"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch identifiers and documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "judg_dict = wetsuite.datacollect.eurlex.fetch_by_resource_type('JUDG') # as of this writing there are 27K results  (referring to roughly 4GB worth of HTML)\n",
        "judg_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store just the fact that the CELEX identifiers exist   \n",
        "#   also the workid they point to, though we don't use that yet\n",
        "for work in judg_dict['results']['bindings']:\n",
        "    try:\n",
        "        celex  = work['celex']['value']\n",
        "        workid = work['work']['value']\n",
        "        judgment_celexes.put(celex, workid)\n",
        "    except KeyError as ke:\n",
        "        print( 'missing %s: %s'%(str(ke), work) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/wetsuite/helpers/notebook.py:103: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  import tqdm.autonotebook  # pylint: disable=C0415\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.009955644607543945,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "fetching pages...",
              "rate": null,
              "total": 23331,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1ad6841ca944445bb21d3fa16ffd143",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "fetching pages...:   0%|          | 0/23331 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "404 https://eur-lex.europa.eu/legal-content/NL/ALL/?uri=CELEX:62006TJ0060\n"
          ]
        }
      ],
      "source": [
        "# Fetch the web pages for all those CELEXes, for one or more languages\n",
        "\n",
        "pbar = wetsuite.helpers.notebook.progress_bar( len(judgment_celexes), description='fetching pages...')\n",
        "count_cached, count_fetched = 0, 0\n",
        "\n",
        "for celex in judgment_celexes:\n",
        "    # the /ALL/ page gives more metadata than e.g. AUTO, TXT, though we might be interested in fetching specific-language \n",
        "    for lang, store, url in (\n",
        "        ('nl', judgment_docs_nl, 'https://eur-lex.europa.eu/legal-content/NL/ALL/?uri=CELEX:%s'%celex),\n",
        "        ('en', judgment_docs_en, 'https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX:%s'%celex),\n",
        "    ):\n",
        "        try:\n",
        "            _, was_cached = wetsuite.helpers.localdata.cached_fetch( store, url )\n",
        "            if was_cached:\n",
        "                count_cached += 1\n",
        "            else:\n",
        "                count_fetched += 1\n",
        "                # it seems the server will report overloads as 404, so running it another time should work, but backoff is nicer to the servers\n",
        "                time.sleep( 1 )\n",
        "        except Exception as e:\n",
        "            print( e, url )\n",
        "    \n",
        "    pbar.value += 1\n",
        "    pbar.description = f'{count_fetched} fetched, {count_cached} cached'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test parsing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# this is some debug, we are not storing yet\n",
        "\n",
        "for url in random.sample( judgment_docs_nl.keys(), 10 ): # pick a bunch of random documents, \n",
        "    random_doc = judgment_docs_nl[ url ]\n",
        "    try:\n",
        "        #print(url)\n",
        "        parsed = wetsuite.datacollect.eurlex.extract_html(random_doc)   # that function is where most of the scraping code sits\n",
        "        #pprint.pprint( parsed['text'] )\n",
        "    except Exception as e:\n",
        "        print( url )\n",
        "        raise"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Did that give good text and not error out?   Then we can probably run it on the whole set and store the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "parsed_store = wetsuite.helpers.localdata.LocalKV('eurlex_parsed.db', key_type=str,value_type=str)    # stores CELEX -> json as str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.010073184967041016,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "parsing and storing...",
              "rate": null,
              "total": 23330,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1165f81e60284c199babc46f55a196b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "parsing and storing...:   0%|          | 0/23330 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# parse and store\n",
        "pbar = wetsuite.helpers.notebook.progress_bar( len(judgment_docs_nl), description='parsing and storing...')\n",
        "force = False\n",
        "for url in judgment_docs_nl.keys():\n",
        "    if force   or  url not in parsed_store:\n",
        "        docbytes = judgment_docs_nl[ url ]\n",
        "        try:\n",
        "            parsed = wetsuite.datacollect.eurlex.extract_html( docbytes )\n",
        "            parsed_store.put( url, json.dumps( parsed ) )\n",
        "        except Exception as e:\n",
        "            print( url )\n",
        "            pprint.pprint( parsed )\n",
        "            raise\n",
        "    pbar.value += 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Regulations\n",
        "\n",
        "Basically the same as above (so look for code comments above), but for regulations instead.\n",
        "\n",
        "TODO: actually finish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "reg_celexes = wetsuite.helpers.localdata.LocalKV('eurlex_reg_celex_workid.db', key_type=str,value_type=str)     # stores CELEX -> work id       (mostly just for the CELEX)\n",
        "reg_docs_en = wetsuite.helpers.localdata.LocalKV('eurlex_reg_en.db', key_type=str,value_type=bytes)   # stores url -> html document\n",
        "reg_docs_nl = wetsuite.helpers.localdata.LocalKV('eurlex_reg_nl.db', key_type=str,value_type=bytes)   # stores url -> html document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch current list\n",
        "reg_dict = wetsuite.datacollect.eurlex.fetch_by_resource_type('REG') # as of this writing there are 130K results (roughly GB worth of HTML)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# take that fetched state and update (mainly) the fact that the CELEX identifiers exist   (also the workid they point to, though we don't use that yet)\n",
        "for work in reg_dict['results']['bindings']:\n",
        "    try:\n",
        "        celex  = work['celex']['value']\n",
        "        workid = work['work']['value']\n",
        "        reg_celexes.put(celex, workid, commit=False)  \n",
        "    except KeyError as ke:\n",
        "        print( 'missing %s: %s'%(str(ke), work) )\n",
        "reg_celexes.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(130031, 130031)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(reg_docs_nl), len(reg_celexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.010289907455444336,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "fetching pages...",
              "rate": null,
              "total": 130031,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f65f034efafc415b9e1f1fc7629e8a3e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "fetching pages...:   0%|          | 0/130031 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# fetch the web pages for all those CELEXes, for EN\n",
        "\n",
        "pbar = wetsuite.helpers.notebook.progress_bar( len(reg_celexes), description='fetching pages...')\n",
        "\n",
        "for celex in reg_celexes.keys():\n",
        "    # the /ALL/ page gives more metadata than e.g. AUTO, TXT, though we might be interested in fetching specific-language \n",
        "    if 1:\n",
        "        url = 'https://eur-lex.europa.eu/legal-content/NL/ALL/?uri=CELEX:%s'%celex\n",
        "        try:\n",
        "            wetsuite.helpers.localdata.cached_fetch( reg_docs_nl, url )\n",
        "            #print(url)\n",
        "        except Exception as e:\n",
        "            print( e, url )\n",
        "\n",
        "    if 0:\n",
        "        url = 'https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX:%s'%celex\n",
        "        try:\n",
        "            wetsuite.helpers.localdata.cached_fetch( reg_docs_en, url )\n",
        "        except Exception as e: # it seems the server will report overloads as 404, so running it another time should\n",
        "            print( e, url )\n",
        "    pbar.value += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['102551']\n",
            "['126317']\n",
            "['1984/1083/CNS']\n",
            "['177633']\n",
            "['114925']\n",
            "['123924']\n",
            "['129076']\n",
            "['2004/0157/COD']\n",
            "['198520']\n"
          ]
        }
      ],
      "source": [
        "# test parsing again\n",
        "\n",
        "selection = random.sample( reg_docs_nl.keys(), 100 )  # pick 100 random documents\n",
        "\n",
        "   \n",
        "for url in selection: \n",
        "    random_doc = reg_docs_nl.get( url )\n",
        "    try:\n",
        "        #print(url)\n",
        "        parsed = wetsuite.datacollect.eurlex.extract_html(random_doc)   # that function is where most of the scraping code sits\n",
        "        #if random.uniform(0,1)<0.05:\n",
        "        #    pprint.pprint( parsed )\n",
        "        #pprint.pprint( parsed['text'] )\n",
        "    except Exception as e:\n",
        "        print( url )\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
