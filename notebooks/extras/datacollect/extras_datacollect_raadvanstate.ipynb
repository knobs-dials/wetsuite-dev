{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, datetime, time, json\n",
    "\n",
    "import bs4\n",
    "\n",
    "import wetsuite.helpers.localdata\n",
    "import wetsuite.helpers.net\n",
    "import wetsuite.helpers.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs_fetched = wetsuite.helpers.localdata.open_store('rvs_fetched.db', str, bytes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go through the webpage list of all advices; fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpage  = 0        # will be set to the real number from the first (well, every) page we fetch\n",
    "cur_page = 0        # zero-based counting in the pagination\n",
    "\n",
    "while cur_page <= maxpage:\n",
    "    page_url = 'https://www.raadvanstate.nl/adviezen/?pager_page=%d&pager_rows=100'%cur_page\n",
    "    page_data = wetsuite.helpers.net.download(page_url)\n",
    "    soup = bs4.BeautifulSoup( page_data, 'lxml' )\n",
    "\n",
    "    # get the amount of pages, from the pagination links\n",
    "    pager_links = soup.select('a.pager_step')\n",
    "    for pager_link in pager_links:\n",
    "        try:\n",
    "            data_page = pager_link.get('data-page')\n",
    "            maxpage = max( maxpage, int(data_page))\n",
    "        except ValueError as ve:\n",
    "            print( \"WARNING: didn't understand %r as page number (%s)\"%(data_page, ve) )\n",
    "\n",
    "    print( \"\\nPAGE %d of %d\"%( cur_page+1, maxpage+1 ) ) # numbering is zero-based,  print out one-based for humans\n",
    "\n",
    "\n",
    "    # fetch all links to specific case detail pages\n",
    "    for detail_page_a in soup.select('a[href*=\"/adviezen/@\"]'):   # links that look like https://www.raadvanstate.nl/adviezen/@133837/w02-22-00162-ii/\n",
    "        detail_page_url = detail_page_a.get('href') # these are already absolute  (otherwise we'd have to urljoin them)\n",
    "\n",
    "        if '#' in detail_page_url:\n",
    "            detail_page_url = detail_page_url.split('#',1)[0]\n",
    "\n",
    "        # there seems to be nothing on the search result page that isn't on the detail pages, so we can just fetch now, and handle each individually later\n",
    "        # Such pages contain both samenvatting and volledigetekst, and the links to them are just #hash that presumably scripting pays attention to\n",
    "        bytedata, is_cached = wetsuite.helpers.localdata.cached_fetch( rvs_fetched, detail_page_url )\n",
    "        if not is_cached:\n",
    "            print(' FETCHED  - ', end='')\n",
    "            time.sleep(0.2)\n",
    "        else:\n",
    "            print(' CACHED   - ', end='')\n",
    "\n",
    "        # debug - print a list of URLs with their titles\n",
    "        print('%-70s '%detail_page_url, end='')\n",
    "        print('%s '%detail_page_a.text)\n",
    "\n",
    "    cur_page += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go through fetched pages, massage into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helpers_progressbar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/var/www/coding/wetsuite/notebooks/miscellany/datacollect_raadvanstate.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3137382e323139222c2275736572223a227363617266626f79227d/var/www/coding/wetsuite/notebooks/miscellany/datacollect_raadvanstate.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3137382e323139222c2275736572223a227363617266626f79227d/var/www/coding/wetsuite/notebooks/miscellany/datacollect_raadvanstate.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mPATH\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mPATH\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:/var/www/coding\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3137382e323139222c2275736572223a227363617266626f79227d/var/www/coding/wetsuite/notebooks/miscellany/datacollect_raadvanstate.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhelpers_progressbar\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3137382e323139222c2275736572223a227363617266626f79227d/var/www/coding/wetsuite/notebooks/miscellany/datacollect_raadvanstate.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m pb \u001b[39m=\u001b[39m helpers_progressbar\u001b[39m.\u001b[39mProgressBar(\u001b[39mlen\u001b[39m(rvs_fetched))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e3137382e323139222c2275736572223a227363617266626f79227d/var/www/coding/wetsuite/notebooks/miscellany/datacollect_raadvanstate.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m page_url \u001b[39min\u001b[39;00m rvs_fetched\u001b[39m.\u001b[39mkeys():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'helpers_progressbar'"
     ]
    }
   ],
   "source": [
    "dataset = {}\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] = os.environ['PATH']+':/var/www/coding'\n",
    "import helpers_progressbar\n",
    "pb = helpers_progressbar.ProgressBar(len(rvs_fetched))\n",
    "\n",
    "for page_url in rvs_fetched.keys():\n",
    "    pb.increment()\n",
    "    pb.simple()\n",
    "    page_data = rvs_fetched[page_url]\n",
    "\n",
    "    kenmerk = None\n",
    "    soup = bs4.BeautifulSoup( page_data, 'lxml' )\n",
    "\n",
    "    title = soup.select('div.rol-paginatitel h1.grid-title')[0].text        \n",
    "\n",
    "    meta = {'trefwoorden':[]}\n",
    "    links = []\n",
    "    #images = []\n",
    "\n",
    "    last_dt = ''\n",
    "    metadata_blok_dl = soup.find('div',  attrs={'class':re.compile(r'\\brol-metadata-blok\\b')}).find('dl')\n",
    "    for ch in metadata_blok_dl.findAll(['dt','dd']):\n",
    "        if ch.name == 'dt':\n",
    "            last_dt = ch.text.strip()\n",
    "        elif ch.name == 'dd':\n",
    "            val = ch.text.strip()\n",
    "            try:\n",
    "                maanden = {\n",
    "                    'januari':1, 'februari':2, 'maart':3, 'april':4,  'mei':5,  'juni':6, 'juli':7,\n",
    "                    'augustus':8, 'september':9, 'october':10, 'oktober':10, 'november':11, 'december':12,\n",
    "                }\n",
    "                if 'Datum' in last_dt:\n",
    "                    day, month, year = val.split()\n",
    "                    if month.lower() in maanden:\n",
    "                        month = '%02d'%maanden[month.lower()]\n",
    "                    else:\n",
    "                        raise ValueError('not doing half a parse')\n",
    "                    val = '%04d-%s-%02d'%(int(year), month, int(day))\n",
    "\n",
    "                meta[last_dt] = val\n",
    "\n",
    "            except ValueError as e: # assume the date didn't manage to parse - we set the string as we got it\n",
    "                print(\"Didn't parse %r as date: %e\"%(val, e))\n",
    "                meta[last_dt] = val \n",
    "                \n",
    "        else:\n",
    "            raise ValueError(\"Don't understand dd child %r\"%ch.name)\n",
    "\n",
    "    trefwoorden_ul = soup.find('ul',  attrs={'class':re.compile(r'\\btrefwoorden\\b')})\n",
    "    for ch in trefwoorden_ul.findAll('li'):\n",
    "        meta['trefwoorden'].append( ch.text.strip() )\n",
    "\n",
    "    kenmerk = meta['Kenmerk']\n",
    "\n",
    "    if 0:\n",
    "        samenvatting_pars = []\n",
    "        samenvatting   = soup.find(id='samenvatting')\n",
    "        if samenvatting is not None:\n",
    "            samenvatting_div = samenvatting.find(attrs={'class':re.compile(r'\\biprox-content\\b')})\n",
    "            for ch in samenvatting_div.children:\n",
    "                if ch.name not in ('p h2 h3 ol'.split()):\n",
    "                    pass\n",
    "                    #print( [ch.name, ch]  )\n",
    "                else:\n",
    "                    samenvatting_pars.append( ' '.join( ch.findAll(text=True) ) )\n",
    "        \n",
    "    pars = []\n",
    "    curpar = []\n",
    "    def flush_curpar():\n",
    "        global pars, curpar\n",
    "        if len(curpar)>0:\n",
    "            pars.append( ' '.join(curpar) )\n",
    "            curpar=[]\n",
    "\n",
    "    # note: headers generally are a <strong> - but they may not be part of the <p> you think.\n",
    "    volledigetekst = soup.find(id='volledigetekst')\n",
    "    if volledigetekst is not None:\n",
    "        volledigetekst_div = volledigetekst.find(attrs={'class':re.compile(r'\\biprox-content\\b')})\n",
    "        #print( volledigetekst )\n",
    "        for ch in volledigetekst_div.children:\n",
    "            if ch.name not in ('p'.split()):\n",
    "                pass\n",
    "                #print( [ch.name, ch]  )\n",
    "            else:\n",
    "                # CONSIDER: separate voetnoten\n",
    "                for thing in ch.contents:\n",
    "                    if type(thing) is bs4.NavigableString:\n",
    "                        curpar.append( str(thing) ) # TODO: is that the best way to get the text?\n",
    "\n",
    "                    elif thing.name == 'em':\n",
    "                        curpar.append( thing.text )\n",
    "\n",
    "                    elif thing.name == 'span': # TODO: check what these actually are\n",
    "                        curpar.append( thing.text )\n",
    "\n",
    "                    elif thing.name == 'sub': # TODO: current code will separate that, which it probably shouldn't be.\n",
    "                        curpar.append( thing.text )\n",
    "\n",
    "                    elif thing.name == 'br':\n",
    "                        flush_curpar()\n",
    "\n",
    "                    elif thing.name == 'a':\n",
    "                        links.append( thing.get('href') )\n",
    "                        curpar.append( thing.text )\n",
    "\n",
    "                    elif thing.name == 'img':\n",
    "                        pass\n",
    "                        #img_abs = urllib.parse.urljoin( page_url , thing.get('src'))  # TODO: look at all the attributes if we want to use it\n",
    "\n",
    "                    elif thing.name == 'strong':\n",
    "                        # see if it's a header\n",
    "                        flush_curpar()\n",
    "                        curpar.append( \n",
    "                                #'[%s]'%\n",
    "                                thing.text\n",
    "                        )\n",
    "                        flush_curpar()\n",
    "\n",
    "                    #elif type(thing) is bs4.NavigableString:\n",
    "                    #    curpar.append( thing.text )\n",
    "                    else:\n",
    "                        raise ValueError( \"Don't know what to do with %r\"%thing)\n",
    "                flush_curpar()\n",
    "\n",
    "        #for par in pars:       \n",
    "        #   print( par )\n",
    "        #    print()\n",
    "\n",
    "    if kenmerk is None:\n",
    "        raise ValueError( \"No kenmerk for %u\"%page_url )\n",
    "\n",
    "    dataset[kenmerk] = {\n",
    "        'title':title,\n",
    "        'url':page_url,\n",
    "        'meta':meta,\n",
    "        'body':pars,\n",
    "        'links':links,\n",
    "    }\n",
    "    #pprint.pprint(dataset[kenmerk])\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write dataset into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset= {\n",
    "    'descrition':'''\n",
    "These are a parsed form of Raad van State (state council) advice,\n",
    "specifically the set of documents under https://www.raadvanstate.nl/adviezen\n",
    "scraped into plain-text documents. \n",
    "\n",
    "Items look like:    \n",
    "'W01.19.0027/I': {'title': 'Voorstel van wet van het lid [...]',\n",
    "                'url': 'https://www.raadvanstate.nl/adviezen/@113252/w01-19-0027/'\n",
    "                'body': ['Bij brief van de voorzitter van de [...]',  # a list of paragraph-like fragments. \n",
    "                        ],\n",
    "                'links': ['http://www.rijksoverheid.nl/documenten/rapporten/2015/11/19/het-lokale-referendum-in-Nederland,).(156'],\n",
    "                'meta': {'Kenmerk': 'W01.19.0027/I',\n",
    "                            'trefwoorden': ['Algemene zaken', 'Initiatiefwet']\n",
    "                            'Datum aanhangig': '2019-01-30',\n",
    "                            'Datum advies': '2019-09-18',\n",
    "                            'Datum vastgesteld': '2019-09-18',\n",
    "                            'Datum publicatie': '2019-10-28',\n",
    "                            'Vindplaats': 'Kamerstukken II 2019/20, 35129, nr. 4', #  if at scraping time this was not settled, it will probably say \"Website Raad van State\" instead\n",
    "                        },\n",
    "                },\n",
    "\n",
    "This dataset generated on %s\n",
    "    '''%datetime.date.today().strftime('%Y-%m-%d'),\n",
    "    'data':dataset,\n",
    "}\n",
    "\n",
    "\n",
    "with open('raadvanstate_adviezen.json', 'w') as wf:\n",
    "    wf.write( json.dumps(dataset) ) \n",
    "wf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
