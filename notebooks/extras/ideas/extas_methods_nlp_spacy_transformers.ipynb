{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers in spacy\n",
    "\n",
    "Why transformers?\n",
    "\n",
    "From the context of classical spacy models: \n",
    " - They are better at noticing context, so presumably any trainable component will train a little better.\n",
    " - They are also better at making something of unknown words, which is useful in languages that make new words by combining morphemes ([agglutination](https://en.wikipedia.org/wiki/Agglutination)), like Dutch does.\n",
    "\n",
    "\n",
    "Consider an transformer-based model that was already molded into spacy-style usability, like [en_core_web_trf](https://spacy.io/models/en#en_core_web_trf)\n",
    "\n",
    "Now say you want a transformer-based spacy model one for Dutch.  Transformers for ducth exist, ***but*** things are not quite that simple."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some distinctions you may care about"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now maybe you've heard that transformers are cooler and you've found [huggingface.co/models](https://huggingface.co/models) which is even from the same company, [explosion.ai](https://explosion.ai/) ([behind at least](https://explosion.ai/software#thinc) [spacy](https://spacy.io/), [huggingface](https://huggingface.co/), [prodigy](https://prodi.gy/) and libraries like [`transformers`](https://github.com/huggingface/transformers) and [thinc](https://thinc.ai/)).\n",
    "\n",
    "However, [huggingface.co/models](https://huggingface.co/models) is just intended as a central store for a models in a much broader sense (and includes things for text, images, audio, and more; transformer is one architecture among many).   So the spacy models are there too (e.g. [en_core_web_trf](https://huggingface.co/spacy/en_core_web_trf) again).\n",
    "\n",
    "This makes it a little harder to explain where these things are not the same."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On models and models**\n",
    "\n",
    "The models listed at https://spacy.io/models are the ones you can fetch via `spacy download modelname` and then spacy.load().\n",
    "\n",
    "\n",
    "\n",
    "And then there's a `transformers` library, also by them. https://huggingface.co/docs/transformers/index\n",
    "And a `spacy_transformers`\n",
    "\n",
    "\n",
    "\"Is `transformers` like spacy? Can you just use them in spacy?\"\n",
    "\n",
    "Broadly, no.\n",
    "There is a lot of shared approach, e.g. specifying a model through configuration. \n",
    "But the `transformers` library, \n",
    "\n",
    "\n",
    "If it is a spacy model, it will probably tell give you the code to use it.\n",
    "See e.g. [nl_udv25_dutchalpino_trf](https://huggingface.co/explosion/nl_udv25_dutchalpino_trf)'s \"Use in spaCy\" button\n",
    "\n",
    "\n",
    "Is \n",
    "The [huggingface-listed models](https://huggingface.co/models),\n",
    "are their own thing - and broader, also having models for images and audio.\n",
    "\n",
    "Well, the spacy models are **also** here. As are some other models that are **for** spacy \n",
    "\n",
    "It can be confusing that while the same company is behind different collections of models \n",
    "\n",
    "\n",
    "\n",
    "a collection of transformer models (see) and a \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A lot of transformer models aren't made for spacy at all.\n",
    "\n",
    "Maybe you found something that seems to be doing masked language modeling (MLM), or NER. \n",
    "\n",
    "\n",
    "This can be confusing if you didn't do a deep dive into spacy, and just found some code that says it's transformers that, say, just seem to do masking\n",
    "\n",
    "they just can be adapted for use by spacy in that way.\n",
    "\n",
    "\n",
    "\n",
    "\"Can I just drop in a transformer thing and have it deal with words better?\"\n",
    "\n",
    "Broadly, that's the idea.\n",
    "\n",
    "But if you want to make a usable, loadable, tunable spacy model, you'll need to do one deep dive.\n",
    "\n",
    "There's a reason that if you look for \"how to adapt transformers in spacy\"\n",
    "you will usually just find people loading a _trf model and training a classifier.\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=RB9uDpJPZdc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy  \n",
    "english_trf = spacy.load('en_core_web_trf')\n",
    "\n",
    "for pipe_name in english_trf.pipe_names:\n",
    "    print( '==== %s ====\\n%s\\n'%(pipe_name, english_trf.get_pipe(pipe_name).__doc__) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *  \n",
    "nlp = pipeline(\"ner\", model=\"pdelobelle/robbert-v2-dutch-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_0',\n",
       "  'score': 0.61068934,\n",
       "  'index': 1,\n",
       "  'word': 'Wat',\n",
       "  'start': 0,\n",
       "  'end': 3},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.6292593,\n",
       "  'index': 2,\n",
       "  'word': 'Ġzijn',\n",
       "  'start': 4,\n",
       "  'end': 8},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.5361042,\n",
       "  'index': 3,\n",
       "  'word': 'Ġbinnen',\n",
       "  'start': 9,\n",
       "  'end': 15},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.50466686,\n",
       "  'index': 4,\n",
       "  'word': 'wat',\n",
       "  'start': 15,\n",
       "  'end': 18},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.50857574,\n",
       "  'index': 5,\n",
       "  'word': 'erk',\n",
       "  'start': 18,\n",
       "  'end': 21},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.6450409,\n",
       "  'index': 6,\n",
       "  'word': 'erende',\n",
       "  'start': 21,\n",
       "  'end': 27},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.5645136,\n",
       "  'index': 7,\n",
       "  'word': 'Ġlandschaps',\n",
       "  'start': 28,\n",
       "  'end': 38},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.571217,\n",
       "  'index': 8,\n",
       "  'word': 'elementen',\n",
       "  'start': 38,\n",
       "  'end': 47},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.60749924,\n",
       "  'index': 9,\n",
       "  'word': '?',\n",
       "  'start': 47,\n",
       "  'end': 48}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('Wat zijn binnenwaterkerende landschapselementen?')\n",
    "list(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 815, 20, 143, 3063, 1576, 1237, 19472, 8304, 51, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Wat zijn binnenwaterkerende landschapselementen?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/explosion/spaCy/discussions/8243"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_trf_bertbaseuncased_lg\n",
    "\n",
    "import spacy \n",
    "nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_1',\n",
       "  'score': 0.62097406,\n",
       "  'index': 1,\n",
       "  'word': 'F',\n",
       "  'start': 0,\n",
       "  'end': 1},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.5734882,\n",
       "  'index': 2,\n",
       "  'word': 'oo',\n",
       "  'start': 1,\n",
       "  'end': 3},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.5073553,\n",
       "  'index': 3,\n",
       "  'word': 'Ġbar',\n",
       "  'start': 4,\n",
       "  'end': 7},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.5949716,\n",
       "  'index': 4,\n",
       "  'word': 'Ġyou',\n",
       "  'start': 8,\n",
       "  'end': 11}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"Foo bar you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 204, 131, 9, 3975, 2], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Ik ben een kaas')\n",
    "#help(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline('fill-mask', model='GroNLP/bert-base-dutch-cased') # BERTje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on FillMaskPipeline in module transformers.pipelines.fill_mask object:\n",
      "\n",
      "class FillMaskPipeline(transformers.pipelines.base.Pipeline)\n",
      " |  FillMaskPipeline(model: Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')], tokenizer: Union[transformers.tokenization_utils.PreTrainedTokenizer, NoneType] = None, feature_extractor: Union[ForwardRef('SequenceFeatureExtractor'), NoneType] = None, modelcard: Union[transformers.modelcard.ModelCard, NoneType] = None, framework: Union[str, NoneType] = None, task: str = '', args_parser: transformers.pipelines.base.ArgumentHandler = None, device: int = -1, binary_output: bool = False, **kwargs)\n",
      " |  \n",
      " |  Masked language modeling prediction pipeline using any `ModelWithLMHead`. See the [masked language modeling\n",
      " |  examples](../task_summary#masked-language-modeling) for more information.\n",
      " |  \n",
      " |  This mask filling pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
      " |  `\"fill-mask\"`.\n",
      " |  \n",
      " |  The models that this pipeline can use are models that have been trained with a masked language modeling objective,\n",
      " |  which includes the bi-directional models in the library. See the up-to-date list of available models on\n",
      " |  [huggingface.co/models](https://huggingface.co/models?filter=fill-mask).\n",
      " |  \n",
      " |  <Tip>\n",
      " |  \n",
      " |  This pipeline only works for inputs with exactly one token masked. Experimental: We added support for multiple\n",
      " |  masks. The returned values are raw model output, and correspond to disjoint probabilities where one might expect\n",
      " |  joint probabilities (See [discussion](https://github.com/huggingface/transformers/pull/10222)).\n",
      " |  \n",
      " |  </Tip>\n",
      " |  Arguments:\n",
      " |      model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
      " |          The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      " |          [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
      " |      tokenizer ([`PreTrainedTokenizer`]):\n",
      " |          The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
      " |          [`PreTrainedTokenizer`].\n",
      " |      modelcard (`str` or [`ModelCard`], *optional*):\n",
      " |          Model card attributed to the model for this pipeline.\n",
      " |      framework (`str`, *optional*):\n",
      " |          The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      " |          installed.\n",
      " |  \n",
      " |          If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      " |          both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      " |          provided.\n",
      " |      task (`str`, defaults to `\"\"`):\n",
      " |          A task-identifier for the pipeline.\n",
      " |      num_workers (`int`, *optional*, defaults to 8):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
      " |          workers to be used.\n",
      " |      batch_size (`int`, *optional*, defaults to 1):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
      " |          the batch to use, for inference this is not always beneficial, please read [Batching with\n",
      " |          pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
      " |      args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
      " |          Reference to the object in charge of parsing supplied pipeline parameters.\n",
      " |      device (`int`, *optional*, defaults to -1):\n",
      " |          Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
      " |          the associated CUDA device id. You can pass native `torch.device` too.\n",
      " |      binary_output (`bool`, *optional*, defaults to `False`):\n",
      " |          Flag indicating if the output the pipeline should happen in a binary format (i.e., pickle) or as raw text.\n",
      " |  \n",
      " |      top_k (`int`, defaults to 5):\n",
      " |          The number of predictions to return.\n",
      " |      targets (`str` or `List[str]`, *optional*):\n",
      " |          When passed, the model will limit the scores to the passed targets instead of looking up in the whole\n",
      " |          vocab. If the provided targets are not in the model vocab, they will be tokenized and the first resulting\n",
      " |          token will be used (with a warning, and that might be slower).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FillMaskPipeline\n",
      " |      transformers.pipelines.base.Pipeline\n",
      " |      transformers.pipelines.base._ScikitCompat\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, inputs, *args, **kwargs)\n",
      " |      Fill the masked token in the text(s) given as inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |          args (`str` or `List[str]`):\n",
      " |              One or several texts (or one list of prompts) with masked tokens.\n",
      " |          targets (`str` or `List[str]`, *optional*):\n",
      " |              When passed, the model will limit the scores to the passed targets instead of looking up in the whole\n",
      " |              vocab. If the provided targets are not in the model vocab, they will be tokenized and the first\n",
      " |              resulting token will be used (with a warning, and that might be slower).\n",
      " |          top_k (`int`, *optional*):\n",
      " |              When passed, overrides the number of predictions to return.\n",
      " |      \n",
      " |      Return:\n",
      " |          A list or a list of list of `dict`: Each result comes as list of dictionaries with the following keys:\n",
      " |      \n",
      " |          - **sequence** (`str`) -- The corresponding input with the mask token prediction.\n",
      " |          - **score** (`float`) -- The corresponding probability.\n",
      " |          - **token** (`int`) -- The predicted token id (to replace the masked one).\n",
      " |          - **token** (`str`) -- The predicted token (to replace the masked one).\n",
      " |  \n",
      " |  ensure_exactly_one_mask_token(self, model_inputs: Union[List[ForwardRef('GenericTensor')], ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')])\n",
      " |  \n",
      " |  get_masked_index(self, input_ids: Union[List[ForwardRef('GenericTensor')], ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')]) -> numpy.ndarray\n",
      " |  \n",
      " |  get_target_ids(self, targets, top_k=None)\n",
      " |  \n",
      " |  postprocess(self, model_outputs, top_k=5, target_ids=None)\n",
      " |      Postprocess will receive the raw outputs of the `_forward` method, generally tensors, and reformat them into\n",
      " |      something more friendly. Generally it will output a list or a dict or results (containing just strings and\n",
      " |      numbers).\n",
      " |  \n",
      " |  preprocess(self, inputs, return_tensors=None, **preprocess_parameters) -> Dict[str, Union[List[ForwardRef('GenericTensor')], ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')]]\n",
      " |      Preprocess will take the `input_` of a specific pipeline and return a dictionnary of everything necessary for\n",
      " |      `_forward` to run properly. It should contain at least one tensor, but might have arbitrary other items.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.Pipeline:\n",
      " |  \n",
      " |  __init__(self, model: Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')], tokenizer: Union[transformers.tokenization_utils.PreTrainedTokenizer, NoneType] = None, feature_extractor: Union[ForwardRef('SequenceFeatureExtractor'), NoneType] = None, modelcard: Union[transformers.modelcard.ModelCard, NoneType] = None, framework: Union[str, NoneType] = None, task: str = '', args_parser: transformers.pipelines.base.ArgumentHandler = None, device: int = -1, binary_output: bool = False, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  check_model_type(self, supported_models: Union[List[str], dict])\n",
      " |      Check if the model class is in supported by the pipeline.\n",
      " |      \n",
      " |      Args:\n",
      " |          supported_models (`List[str]` or `dict`):\n",
      " |              The list of models supported by the pipeline, or a dictionary with model class values.\n",
      " |  \n",
      " |  device_placement(self)\n",
      " |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Context manager\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Explicitly ask for tensor allocation on CUDA device :0\n",
      " |      pipe = pipeline(..., device=0)\n",
      " |      with pipe.device_placement():\n",
      " |          # Every framework specific tensor allocation will be done on the request device\n",
      " |          output = pipe(...)\n",
      " |      ```\n",
      " |  \n",
      " |  ensure_tensor_on_device(self, **inputs)\n",
      " |      Ensure PyTorch tensors are on the specified device.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (keyword arguments that should be `torch.Tensor`, the rest is ignored):\n",
      " |              The tensors to place on `self.device`.\n",
      " |          Recursive on lists **only**.\n",
      " |      \n",
      " |      Return:\n",
      " |          `Dict[str, torch.Tensor]`: The same as `inputs` but on the proper device.\n",
      " |  \n",
      " |  forward(self, model_inputs, **forward_params)\n",
      " |  \n",
      " |  get_inference_context(self)\n",
      " |  \n",
      " |  get_iterator(self, inputs, num_workers: int, batch_size: int, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  iterate(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  run_multi(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: str)\n",
      " |      Save the pipeline's model and tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str`):\n",
      " |              A path to the directory where to saved. It will be created if it doesn't exist.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.pipelines.base.Pipeline:\n",
      " |  \n",
      " |  default_input_names = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.pipelines.base._ScikitCompat:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pipe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parijs is de [MASK] \n",
      "  -> Parijs is de stad\n",
      "Parijs is de stad  [MASK] \n",
      "  -> Parijs is de stad van\n",
      "Parijs is de stad van  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n",
      "  -> Parijs is de stad van Frankrijk\n",
      "Parijs is de stad van Frankrijk  [MASK] \n"
     ]
    }
   ],
   "source": [
    "import random, re\n",
    "cur = 'Parijs is de [MASK] '\n",
    "\n",
    "for _ in range(30):\n",
    "    print( cur )\n",
    "    res = pipe(cur)\n",
    "    \n",
    "\n",
    "    fres = []\n",
    "    for r in res:\n",
    "        #print(r)\n",
    "        if re.search(r'[A-Za-z]+', r['token_str']) is not None:\n",
    "            #print('ACCEPT: %r'%r['token_str'] )\n",
    "            fres.append(r)\n",
    "        #else:\n",
    "        #    print('SKIP: %r'%r['token_str'])\n",
    "\n",
    "    choice = random.choice( fres)\n",
    "    #choice = random.choice( list(r  for r in res  if len(r['sequence'])>=2 ) )\n",
    "    print ('  -> %s'%choice['sequence'])\n",
    "    tt = choice['sequence'].split()\n",
    "    #rpos = random.randint( max(2, len(tt)-2) , len(tt))\n",
    "    #tt = tt[:rpos] + [' [MASK] '] + tt[rpos:]\n",
    "    tt = tt + [' [MASK] ']\n",
    "    cur = ' '.join( tt )\n",
    "\n",
    "print( cur )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
