{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import collections\n",
    "import re\n",
    "import random\n",
    "\n",
    "import networkx\n",
    "\n",
    "import wetsuite.helpers.meta\n",
    "import wetsuite.helpers.patterns\n",
    "import wetsuite.extras.gerechtcodes\n",
    "import wetsuite.datasets\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wetsuite.datasets.list_datasets()\n",
    "\n",
    "bwb  = wetsuite.datasets.load('bwb-mostrecent-xml').data\n",
    "cvdr = wetsuite.datasets.load('cvdr-mostrecent-xml').data\n",
    "rsnl = wetsuite.datasets.load('rechtspraaknl-struc').data\n",
    "# TODO: even more free-flowing text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some out test data - ECLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goes through Gbytes of text, will take a few minutes\n",
    "reload(wetsuite.helpers.meta)\n",
    "reload(wetsuite.helpers.patterns)\n",
    "\n",
    "mostrefs     = 0\n",
    "mostrefs_url = ''\n",
    "\n",
    "with open('eclitest.txt','w') as eclitestfile:\n",
    "\n",
    "    for store in ( #easier to skip one with comments this way\n",
    "        bwb, \n",
    "        cvdr,\n",
    "        rsnl,\n",
    "    ):\n",
    "\n",
    "        for i, (url, data) in enumerate( store.items() ):\n",
    "            if isinstance(data, dict): # assume rechtspraak rather than the others\n",
    "                data = str(data)  # close enough for now\n",
    "            else: # assume it's a bytes object\n",
    "                data = data.decode('utf8')\n",
    "                \n",
    "            if 1:\n",
    "                # that regexp awkwardly tries to accept more, without too much nonsense\n",
    "                matches = list( re.finditer(r'(?<!:)([eECc][a-zA-Z]+[:](?:[^\\s]{1,10}[:]){2,7}[^\\s]+)', data) ) #intentionally overaccepts at the end\n",
    "                if len(matches) > 1:\n",
    "                    #print( f'============== {url} == {len(matches)} matches ==============')\n",
    "                    for match_object in matches:\n",
    "                        txt = match_object.group(0)\n",
    "                        if 'jci' in txt:\n",
    "                            continue\n",
    "                        #print( match_object , txt )\n",
    "                        msg = f'{txt}\\t{url}\\n'\n",
    "                        eclitestfile.write( msg )\n",
    "                        #print( msg, end=''  )\n",
    "                    eclitestfile.flush()\n",
    "\n",
    "                #if i>20000:\n",
    "                #    break            \n",
    "\n",
    "            if 0: # count distinct sourceref types\n",
    "                import wetsuite.helpers.koop_parse\n",
    "                tree = wetsuite.helpers.etree.fromstring( data.encode('utf8') )\n",
    "\n",
    "                try:\n",
    "                    count_types = collections.defaultdict(int)\n",
    "                    refs = wetsuite.helpers.koop_parse.cvdr_sourcerefs( tree )\n",
    "                    typs = collections.defaultdict(int)\n",
    "                    for ref in refs:\n",
    "                        typs[ref[0]]+=1\n",
    "\n",
    "                    if len(typs) > mostrefs:\n",
    "                        mostrefs = len(typs)\n",
    "                        mostrefs_url = url\n",
    "                        print( 'New best: %s for %r'%(mostrefs, mostrefs_url) )\n",
    "                except Exception as e:\n",
    "                    print( e )\n",
    "                    pass\n",
    "\n",
    "                #if i>200:\n",
    "                #    break\n",
    "\n",
    "            if 0: # print all URLs with any extrefs\n",
    "                if '<extref' in data:\n",
    "                    print(url)\n",
    "                    break\n",
    "\n",
    "                #if i>200:\n",
    "                #    break\n",
    "\n",
    "            if 0: # test extraction of most identifier-type references in the text\n",
    "                refs = wetsuite.helpers.patterns.find_identifier_references( data, nonidentifier=False ) \n",
    "                if len(refs)>0:\n",
    "                    print( '================== %s =================='%url )\n",
    "                    for ref in refs:\n",
    "                        print( '  %s: %r'%(ref.pop('type'), ref.pop('text')) )\n",
    "                        pprint.pprint(ref, indent=5)\n",
    "\n",
    "                #if i>200:\n",
    "                #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a subset of that ECLI test data \n",
    "(we don't need that many good examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(wetsuite.helpers.meta)\n",
    "reload(wetsuite.helpers.patterns)\n",
    "reload(wetsuite.extras.gerechtcodes)\n",
    "\n",
    "#count_str = collections.defaultdict(int)\n",
    "#count_bad     = 0\n",
    "\n",
    "good = []\n",
    "bad  = []\n",
    "\n",
    "with open('eclitest.txt','r') as eclitestfile:\n",
    "    for line in eclitestfile:\n",
    "        text, url = line.rstrip('\\n').rsplit('\\t', 1)\n",
    "        \n",
    "        try:\n",
    "            parsed = wetsuite.helpers.meta.parse_ecli(text)\n",
    "            #if parsed['country_code'].upper()=='NL':\n",
    "            #    cc = parsed['court_code']\n",
    "            #    if wetsuite.extras.gerechtcodes.case_insensitive_lookup(cc) is None:\n",
    "            #        count_str[cc] += 1\n",
    "            good.append( (text, url) )\n",
    "        except ValueError as ve:\n",
    "            bad.append( (text, url) )\n",
    "            #count_bad += 1\n",
    "\n",
    "            if 0:\n",
    "                sve = str(ve)\n",
    "                if 'separated by four colons' in sve:\n",
    "                    continue\n",
    "                if 'First' in sve:\n",
    "                    continue\n",
    "                if 'country' in sve:\n",
    "                    continue\n",
    "                if 'not look' in sve:\n",
    "                    continue\n",
    "\n",
    "            #count_str[text.split(':')[0]] += 1\n",
    "            print(text, ve, url)\n",
    "\n",
    "if 0:\n",
    "    it = list( count_str.items() )\n",
    "    it.sort(key=lambda x:x[1], reverse=True)\n",
    "    for k, c in it:\n",
    "        print('%5r %s'%(k, c))\n",
    "\n",
    "print( len(good), len(bad) )\n",
    "\n",
    "with open('good_ecli.txt','w') as good_ecli_file:\n",
    "    for text, url in random.sample(good, 20000):\n",
    "        good_ecli_file.write(f'{text}\\t{url}\\n')\n",
    "\n",
    "with open('bad_ecli.txt','w') as bad_ecli_file:\n",
    "    for text, url in random.sample(bad, 20000):\n",
    "        bad_ecli_file.write(f'{text}\\t{url}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
