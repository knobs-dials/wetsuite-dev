#!/usr/bin/python3
'''
    Takes a directory of downloadable files, 
    generates the metadata used for the index.
'''
import os
import sys
import glob
import tempfile
import hashlib
import json
import lzma  # >= py3.3

#import wetsuite.helpers.util
import wetsuite.helpers.localdata
import wetsuite.helpers.format



if __name__ == '__main__':

    # a local cache that avoids a bunch of repeated reading on unchanged uploads
    dataset_meta = wetsuite.helpers.localdata.MsgpackKV('dataset_meta.db')
    print("Reading data from %r"%dataset_meta.path)


    args = sys.argv[1:]
    if len(args) != 1:
        print("We want one argument, a directory to look for files in")
        sys.exit(-1)

    in_dir           = args[0]


    dataset_index    = {}                # this is headed for a json file to be served
    dataset_index_fn = os.path.join( in_dir, 'index.json' )  # ...and where it will go

    fns = glob.glob( os.path.join( os.path.abspath(in_dir), '*') ) # *xz
    for fn in sorted(fns):
        ffn = os.path.join( in_dir, fn )
        basename = os.path.basename(fn)
        if basename == 'index.json':
            continue
        #print(' ====== %r ====== '%basename)

        stob = os.stat(ffn)

        #if stob.st_size > 500000000:
        #    print("LARGE, skip during this debug")
        #    continue

        key = '__/__'.join( [ ffn, str( stob.st_size ), str( int(stob.st_mtime) ) ] )


        if key in dataset_meta:
            print( "NOCALC, we know about %r"%ffn)
            stored_dict = dataset_meta.get( key )

        else:  # key not in dataset_meta:
            stored_dict = {
                'basename':             basename,
                'compressed_size':      stob.st_size,
                'compressed_size_human':'%sB'%wetsuite.helpers.format.kmgtp(stob.st_size),
                'mtime':                int(stob.st_mtime),
            }

            filetype = None
            if ffn.endswith('.xz'):
                filetype = 'xz'

            else:
                with open(ffn, 'rb') as f:
                    firstbytes = f.read(20)
                if firstbytes[:7] == b'\xfd7zXZ\x00\x00':
                    filetype='xz'
                elif firstbytes[:15] == b'SQLite format 3': # ideally doesn't happen here, but we will probably end up sharing this logic elsewhere
                    filetype='sqlite3'
                elif firstbytes.lstrip().startswith(b'{'):
                    filetype='json'

            #print( "  uncompressing %s to get its real size, please wait"%ffn )

            if filetype == 'xz':        # CONSIDER: bz2, gz?
                stored_dict['type'] = 'xz-sqlite3'
                print( f"DO XZ {repr(ffn)}" )

                s1h = hashlib.sha1()

                print("\nDETERMINE uncompressed size of %r"%ffn)
                uncompressed_data_size_bytes = 0
                with tempfile.NamedTemporaryFile(delete=False) as temp_file:
                    try:
                        # the main reason we do this is description, maybe think of a more efficient way that doesn't rely on lots of storage being there
                        print( "    into temporary file %r"%temp_file.name )
                        first100000_hash = None
                        with lzma.open( ffn ) as compressed_file_object:
                            while True:
                                data = compressed_file_object.read( 2*1048576 )
                                # a large-ish 2MB for decent speed, should be at least 100k for the next bit:
                                if first100000_hash is None:
                                    s1h1 = hashlib.sha1()
                                    s1h1.update(data[:100000])
                                    first100000_hash = s1h1.hexdigest()

                                if len(data) == 0:
                                    break
                                temp_file.write(data) # write uncompressed data (here mostly to get size and description)
                                s1h.update(data)      # make a hash of the whole thing
                                uncompressed_data_size_bytes += len( data )
                                #print( "\r    decompressing... %3sB    "%(
                                #    wetsuite.helpers.format.kmgtp( uncompressed_data_size_bytes, kilo=1024 ), ),
                                #    end='', file=sys.stderr )
                        stored_dict['uncompressed_size'] = uncompressed_data_size_bytes
                        stored_dict['uncompressed_sha1hex'] = s1h.hexdigest()
                        stored_dict['first100000_sha1hex'] = first100000_hash
                        
                        with wetsuite.helpers.localdata.LocalKV( temp_file.name, None,None, read_only=True ) as tempstore:
                            stored_dict['description']        = tempstore._get_meta('description', missing_as_none=True)
                            stored_dict['description_shyort'] = tempstore._get_meta('description', missing_as_none=True)
                    finally:
                        # It looks like NamedTemporaryFile(delete=True) doesn't always honor that delete (presumably because we can pre-empty a gc-caused clean?)
                        # try to ensure cleanup happend
                        if os.path.exists( temp_file.name ):
                            os.unlink( temp_file.name )

            elif filetype == 'sqlite3': # TODO: remove need (we want downloads to be compressed always)
                stored_dict['type'] = 'sqlite3'
                print( "DO UNCOMPRESSED SQLITE3 %r"%ffn )

                stored_dict['uncompressed_size'] = stored_dict['compressed_size']

                with wetsuite.helpers.localdata.LocalKV( ffn, None,None, read_only=True ) as tstore:
                    stored_dict['description']       = tstore._get_meta('description',       missing_as_none=True)
                    stored_dict['description_short'] = tstore._get_meta('description_short', missing_as_none=True)


            elif filetype == 'json': # CONSIDER: remove need (we want downloads to be compressed always)
                stored_dict['type'] = 'json'
                print( f"DO JSON {repr(ffn)}")

                stored_dict['uncompressed_size'] = stored_dict['compressed_size']


            else:
                print( f"TODO: handle {str(filetype)} {repr(ffn)}" )
                continue


            dataset_meta.put(key, stored_dict)


        ## Either way, describe the thing in the index
        dataset_name = basename.split('.',1)[0]

        dataset_item = {}
        dataset_item['name']                 = dataset_name
        dataset_item['url']                  = 'https://wetsuite.knobs-dials.com/datasets/%s'%basename
        dataset_item['version']              = stored_dict.get('version', '(preliminary)')
        dataset_item['type']                 = stored_dict.get('type',       'unknown')
        dataset_item['description_short']    = stored_dict.get('description_short', '')
        dataset_item['description']          = stored_dict.get('description',       '')
        dataset_item['download_size']        = stored_dict.get('compressed_size',   -1)
        dataset_item['real_size']            = stored_dict.get('uncompressed_size', -1)
        dataset_item['download_size_human']  = '%sB'%wetsuite.helpers.format.kmgtp( stored_dict.get('compressed_size'),   kilo=1024 )
        dataset_item['real_size_human']      = '%sB'%wetsuite.helpers.format.kmgtp( stored_dict.get('uncompressed_size'), kilo=1024 )
        dataset_item['uncompressed_sha1hex'] = stored_dict.get('uncompressed_sha1hex', None)
        dataset_item['first100000_sha1hex']  = stored_dict.get('first100000_sha1hex',  None)

        dataset_index[dataset_name] = dataset_item



print( "Writing %r"%dataset_index_fn )
with open( dataset_index_fn, 'w', encoding='utf8') as wf:
    wf.write( json.dumps( dataset_index ) )
